{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Simplification"
      ],
      "metadata": {
        "id": "Flk2O9OpF8M_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BISB2shmAU5o",
        "outputId": "17270295-fe6c-4771-a78a-ee6d8d103e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/muss\n",
            "Collecting easse@ git+https://github.com/feralvam/easse.git\n",
            "  Cloning https://github.com/feralvam/easse.git to /tmp/pip-install-5i_37kh0/easse_dee4896aeb0f4cae9b03e337db20ce61\n",
            "  Running command git clone -q https://github.com/feralvam/easse.git /tmp/pip-install-5i_37kh0/easse_dee4896aeb0f4cae9b03e337db20ce61\n",
            "Collecting kenlm@ git+https://github.com/kpu/kenlm.git\n",
            "  Cloning https://github.com/kpu/kenlm.git to /tmp/pip-install-5i_37kh0/kenlm_c4853cc049574e5cbdc06edcc131135b\n",
            "  Running command git clone -q https://github.com/kpu/kenlm.git /tmp/pip-install-5i_37kh0/kenlm_c4853cc049574e5cbdc06edcc131135b\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.3.5)\n",
            "Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (3.7)\n",
            "Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (4.64.1)\n",
            "Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.0.post1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.7.3)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (3.1.29)\n",
            "Requirement already satisfied: spacy<3,>=2.1.3 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (2.3.8)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.2.0)\n",
            "Requirement already satisfied: python-Levenshtein>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.20.8)\n",
            "Requirement already satisfied: fairseq==0.10.2 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.10.2)\n",
            "Requirement already satisfied: truecase in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.0.14)\n",
            "Requirement already satisfied: sentencepiece>=0.1.83 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.1.97)\n",
            "Requirement already satisfied: imohash>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.0.4)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (5.2.0)\n",
            "Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.3.6)\n",
            "Requirement already satisfied: submitit in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.4.5)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.7.2)\n",
            "Requirement already satisfied: sacremoses>=0.0.38 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.0.53)\n",
            "Requirement already satisfied: nevergrad>=0.4.0.post3 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.5.0)\n",
            "Requirement already satisfied: editdistance>=0.5.3 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.5.3)\n",
            "Requirement already satisfied: tokenizers>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (0.12.1)\n",
            "Requirement already satisfied: cachier>=1.2.8 in /usr/local/lib/python3.8/dist-packages (from muss==1.0) (1.5.4)\n",
            "Collecting tseval@ git+https://github.com/facebookresearch/text-simplification-evaluation.git@main\n",
            "  Cloning https://github.com/facebookresearch/text-simplification-evaluation.git (to revision main) to /tmp/pip-install-5i_37kh0/tseval_c3bbbe49f8be4726b4d53e2b1125cc3f\n",
            "  Running command git clone -q https://github.com/facebookresearch/text-simplification-evaluation.git /tmp/pip-install-5i_37kh0/tseval_c3bbbe49f8be4726b4d53e2b1125cc3f\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.23.0)\n",
            "Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.3.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.11.2)\n",
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.2.0)\n",
            "Requirement already satisfied: yattag in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.14.0)\n",
            "Requirement already satisfied: plotly>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (5.5.0)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.3.12)\n",
            "Requirement already satisfied: simalign in /usr/local/lib/python3.8/dist-packages (from easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.3)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.8/dist-packages (from fairseq==0.10.2->muss==1.0) (1.2.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from fairseq==0.10.2->muss==1.0) (1.15.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.8/dist-packages (from fairseq==0.10.2->muss==1.0) (0.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from fairseq==0.10.2->muss==1.0) (0.29.32)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from fairseq==0.10.2->muss==1.0) (2022.6.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from cachier>=1.2.8->muss==1.0) (0.1.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from cachier>=1.2.8->muss==1.0) (2.6.0)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.8/dist-packages (from cachier>=1.2.8->muss==1.0) (2.1.9)\n",
            "Requirement already satisfied: varint>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from imohash>=1.0.4->muss==1.0) (1.0.2)\n",
            "Requirement already satisfied: mmh3>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from imohash>=1.0.4->muss==1.0) (3.0.0)\n",
            "Requirement already satisfied: cma>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from nevergrad>=0.4.0.post3->muss==1.0) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from nevergrad>=0.4.0.post3->muss==1.0) (4.1.1)\n",
            "Requirement already satisfied: bayesian-optimization>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from nevergrad>=0.4.0.post3->muss==1.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from bayesian-optimization>=1.2.0->nevergrad>=0.4.0.post3->muss==1.0) (1.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from bayesian-optimization>=1.2.0->nevergrad>=0.4.0.post3->muss==1.0) (0.4.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->muss==1.0) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->muss==1.0) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (8.1.0)\n",
            "Requirement already satisfied: Levenshtein==0.20.8 in /usr/local/lib/python3.8/dist-packages (from python-Levenshtein>=0.12.0->muss==1.0) (0.20.8)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from Levenshtein==0.20.8->python-Levenshtein>=0.12.0->muss==1.0) (2.13.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.21.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=2.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (4.9.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=2.0.0->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.8.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad>=0.4.0.post3->muss==1.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (7.4.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (2.0.7)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3,>=2.1.3->muss==1.0) (0.7.9)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (4.21.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.0.9)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.0->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.0->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers>=3.0.0->bert_score->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.8.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->fairseq==0.10.2->muss==1.0) (2.21)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython->muss==1.0) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython->muss==1.0) (5.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.8/dist-packages (from hydra-core->fairseq==0.10.2->muss==1.0) (4.9.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core->fairseq==0.10.2->muss==1.0) (5.10.0)\n",
            "Requirement already satisfied: omegaconf~=2.2 in /usr/local/lib/python3.8/dist-packages (from hydra-core->fairseq==0.10.2->muss==1.0) (2.2.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core->fairseq==0.10.2->muss==1.0) (3.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (0.11.0)\n",
            "Requirement already satisfied: networkx==2.4 in /usr/local/lib/python3.8/dist-packages (from simalign->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from networkx==2.4->simalign->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (4.4.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from stanfordnlp->easse@ git+https://github.com/feralvam/easse.git->muss==1.0) (3.19.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from submitit->muss==1.0) (1.5.0)\n",
            "Installing collected packages: muss\n",
            "  Attempting uninstall: muss\n",
            "    Found existing installation: muss 1.0\n",
            "    Can't uninstall 'muss'. No files were found to uninstall.\n",
            "  Running setup.py develop for muss\n",
            "Successfully installed muss-1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_md==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 50.8 MB 209 kB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fr_core_news_md (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for fr_core_news_md\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#installing all the dependencies\n",
        "!git clone https://github.com/facebookresearch/muss.git   \n",
        "!cd muss\n",
        "!pip install -e .\n",
        "!python -m spacy download en_core_web_md fr_core_news_md es_core_news_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/simplify.py scripts/examples.en --model-name muss_en_wikilarge_mined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuPJ8NXCA2T6",
        "outputId": "7ce25750-f8f5-4e05-cea8-edc1c44eb447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1625161728 bytes == 0x74328000 @  0x7f19c5ed4b6b 0x7f19c5ef4379 0x7f1944576d57 0x7f1944564bc3 0x7f1996334811 0x5d7f65 0x56376d 0x55e858 0x5d7cf1 0x4eb089 0x5d86fe 0x654d9d 0x58d798 0x4f7763 0x49ca7c 0x55e858 0x5d7cf1 0x5d77c6 0x561051 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x49ced5\n",
            "tcmalloc: large alloc 1625161728 bytes == 0xd5108000 @  0x7f19c5ed4b6b 0x7f19c5ef4379 0x7f1944576d57 0x7f1944564bc3 0x7f1996334811 0x5d7f65 0x56376d 0x55e858 0x5d7cf1 0x4eb089 0x5d86fe 0x654d9d 0x58d798 0x4f7763 0x49ca7c 0x55e858 0x5d7cf1 0x5d77c6 0x561051 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x49ec69 0x55e571 0x5d7cf1 0x49ced5\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.8/dist-packages/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n",
            "--------------------------------------------------------------------------------\n",
            "Original:   When you, remembling a glossy braid of hair, have ascended its summit, the mountain whose slopes are covered with forest mangoes, glowing with ripe fruit, takes on the appearance of a breast of the earth, dark at the centre, the rest pale, worthy to be beheld by a divine couple.\n",
            "Simplified: When you have climbed to the top, the mountain, whose slopes are covered with forest mangoes, glowing with ripe fruit, takes on the appearance of a breast of the earth, dark at the centre, the rest pale, worthy to be beheld by a divine couple.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstractive summarization module"
      ],
      "metadata": {
        "id": "TVLUpa8DClbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, PegasusTokenizer\n",
        "from transformers import BartForConditionalGeneration, PegasusForConditionalGeneration\n",
        "\n",
        "IS_CNNDM = True # whether to use CNNDM dataset or XSum dataset\n",
        "LOWER = False\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = \"When you have climbed to the top, the mountain, whose slopes are covered with forest mangoes, glowing with ripe fruit, takes on the appearance of a breast of the earth, dark at the centre, the rest pale, worthy to be beheld by a divine couple. \"\n",
        "\n",
        "# Load our model checkpoints\n",
        "if IS_CNNDM:\n",
        "    model = BartForConditionalGeneration.from_pretrained('Yale-LILY/brio-cnndm-uncased')\n",
        "    tokenizer = BartTokenizer.from_pretrained('Yale-LILY/brio-cnndm-uncased')\n",
        "else:\n",
        "    model = PegasusForConditionalGeneration.from_pretrained('Yale-LILY/brio-xsum-cased')\n",
        "    tokenizer = PegasusTokenizer.from_pretrained('Yale-LILY/brio-xsum-cased')\n",
        "\n",
        "max_length = 1024 if IS_CNNDM else 512\n",
        "# generation example\n",
        "if LOWER:\n",
        "    article = ARTICLE_TO_SUMMARIZE.lower()\n",
        "else:\n",
        "    article = ARTICLE_TO_SUMMARIZE\n",
        "inputs = tokenizer([article], max_length=max_length, return_tensors=\"pt\", truncation=True)\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs[\"input_ids\"])\n",
        "print(\"The result from abstractive summarization is\")\n",
        "summary=tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpvLk-5tCvVI",
        "outputId": "eec9b94a-f2a6-459e-a5ce-13ef21c94540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result from abstractive summarization is\n",
            "The mountain's slopes are covered with forest mangoes, glowing with ripe fruit. It takes on the appearance of a breast of the earth, dark at the centre and pale at the top.    the mountain is covered in mangoes and mangoes.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence splitting."
      ],
      "metadata": {
        "id": "Zd4sAbpTDO_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import nltk\n",
        "from itertools import chain\n",
        "nltk.download('punkt')\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Concatenate, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import logging\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd gdrive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK205CMtDTU7",
        "outputId": "3ac283c4-2e37-4739-c97b-543df1516162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceSplitter():\n",
        "    def __init__(self,\n",
        "            sentence_length,\n",
        "            lstm_layers_size,\n",
        "            embedding_layer_size,\n",
        "            dropout_rate,\n",
        "            activation,\n",
        "            epochs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.sentence_length = sentence_length\n",
        "        self.lstm_layers_size = lstm_layers_size\n",
        "        self.embedding_layer_size = embedding_layer_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def sent2seq(self, sentence, padding_location=\"pre\"):\n",
        "        sequences = self.tokenizer.texts_to_sequences([sentence])\n",
        "        return pad_sequences(sequences, maxlen=self.sentence_length, dtype='int32', padding=padding_location,\n",
        "            truncating=padding_location, value=0.0).squeeze()\n",
        "\n",
        "    def create_dataset(self, texts: List[str], min_sentence_length_for_splitting=10):\n",
        "        '''\n",
        "        Create positives and negatives pairs of sentences.\n",
        "        Positive pair is a pair of sentences that should be splitted, using nltk.sent_tokenize (without \\n).\n",
        "        Negative pair is a sentence that we split in the middle\n",
        "        '''\n",
        "        positives_pairs = []\n",
        "        negative_pairs = []\n",
        "        for text in texts:\n",
        "            sentences = nltk.sent_tokenize(text)\n",
        "            if len(sentences)>1:\n",
        "                sentences = [nltk.tokenize.word_tokenize(sentence.replace('.', '')) for sentence in sentences]\n",
        "                sentences_pairs = list(zip(sentences[:-1], sentences[1:]))\n",
        "                positives_pairs.append(sentences_pairs)\n",
        "\n",
        "                for negative_sample in sentences:  # split sentence in the middle\n",
        "                    if len(negative_sample) > min_sentence_length_for_splitting:\n",
        "                        first_sent, second_sent = negative_sample[:len(negative_sample) // 2], negative_sample[\n",
        "                                                                                                len(negative_sample) // 2:]\n",
        "                        negative_pairs.append([(first_sent, second_sent)])\n",
        "        \n",
        "        positives_pairs = list(chain.from_iterable(positives_pairs))\n",
        "        negative_pairs = list(chain.from_iterable(negative_pairs))\n",
        "        \n",
        "        return positives_pairs, negative_pairs\n",
        "\n",
        "    def get_cases(self):\n",
        "        myfile = open(\"wikisent2.txt\", \"rt\")\n",
        "        contents = myfile.read() \n",
        "        myfile.close()\n",
        "        return contents.split('\\n')[::8]\n",
        "\n",
        "    def read_dataset(self, texts, positives_pairs, negative_pairs):\n",
        "        self.tokenizer = Tokenizer()\n",
        "        corpus = [[nltk.tokenize.word_tokenize(sentence) for sentence in nltk.sent_tokenize(text)] for text in texts]\n",
        "        corpus =list(chain.from_iterable(corpus)) \n",
        "        self.tokenizer.fit_on_texts(corpus)\n",
        "        positives_pairs = np.array([[self.sent2seq(sent1, padding_location=\"pre\"),\n",
        "                                        self.sent2seq(sent2, padding_location=\"post\")] for sent1, sent2 in\n",
        "                                    positives_pairs])\n",
        "        negative_pairs = np.array([[self.sent2seq(sent1, padding_location=\"pre\"),\n",
        "                                    self.sent2seq(sent2, padding_location=\"post\")] for sent1, sent2 in negative_pairs])\n",
        "        X = np.concatenate((positives_pairs, negative_pairs))\n",
        "        y = np.array([1] * len(positives_pairs) + [0] * len(negative_pairs))\n",
        "        return X, y\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        X: sentences pairs\n",
        "        y: 1- represent splitted sentences, 0- represent merged sentences\n",
        "        '''\n",
        "        weights = y * (len(y) - sum(y)) / sum(y)  # positive weight\n",
        "        weights[weights == 0] = 1  # negative weight\n",
        "        logging.info(f\"Number of positives: {len(y[y == 1])}, Number of negatives: {len(y[y == 0])}\")\n",
        "        vocab_size = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # first sentence forward\n",
        "        forward_model_input = Input(shape=(X.shape[-1],))\n",
        "        forward_model = Embedding(vocab_size, self.embedding_layer_size)(forward_model_input)\n",
        "        forward_model = self.add_lstm_layers(forward_model, go_backwards=False)\n",
        "\n",
        "        # second sentence backward\n",
        "        backward_model_input = Input(shape=(X.shape[-1],))\n",
        "        backward_model = Embedding(vocab_size, self.embedding_layer_size)(backward_model_input)\n",
        "        backward_model = self.add_lstm_layers(backward_model, go_backwards=True)\n",
        "\n",
        "        model_concatenated = Concatenate()([forward_model, backward_model])\n",
        "        model_concatenated = Dense(self.lstm_layers_size[-1] * 2, activation=self.activation)(model_concatenated)\n",
        "\n",
        "        model_concatenated = Dense(1, activation='sigmoid')(model_concatenated)\n",
        "        self.model = Model(inputs=[forward_model_input, backward_model_input], outputs=model_concatenated)\n",
        "\n",
        "        logging.info(f\"Dataset shape: {X.shape}\")\n",
        "        self.model.summary(print_fn=logging.getLogger(__name__).info)\n",
        "\n",
        "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        self.model.fit([X[:, 0, :], X[:, 1, :]], y, batch_size=256, epochs=self.epochs, shuffle=True,\n",
        "            sample_weight=weights)\n",
        "    def add_lstm_layers(self, model, go_backwards=False):\n",
        "        for i, layer_dim in enumerate(self.lstm_layers_size):\n",
        "            if i == len(self.lstm_layers_size) - 1:  # last layer\n",
        "                model = LSTM(layer_dim, return_sequences=False, dropout=self.dropout_rate,\n",
        "                    go_backwards=go_backwards)(model)\n",
        "            else:\n",
        "                model = LSTM(layer_dim, return_sequences=True, dropout=self.dropout_rate,\n",
        "                    go_backwards=go_backwards)(model)\n",
        "        return model\n",
        "    \n",
        "    def transform(self, sentences):\n",
        "        if len(sentences) == 0:\n",
        "            return []\n",
        "        splitted_sentences = [sentences[0]]\n",
        "        for sentence in sentences[1:]:\n",
        "            previous_sentence = splitted_sentences.pop()\n",
        "            pair_sentences = np.array([self.sent2seq(nltk.tokenize.word_tokenize(previous_sentence), padding_location=\"pre\"),\n",
        "                                       self.sent2seq(nltk.tokenize.word_tokenize(sentence), padding_location=\"post\")])\n",
        "\n",
        "            prob_split= self.model.predict([pair_sentences[0, :].reshape(1, self.sentence_length),\n",
        "                                                 pair_sentences[1, :].reshape(1, self.sentence_length)])[:, 0][0]\n",
        "            if prob_split > 0.5:\n",
        "                splitted_sentences += [previous_sentence, sentence]\n",
        "            else: #merge\n",
        "                splitted_sentences += [(previous_sentence + ' ' + sentence)]\n",
        "\n",
        "        return splitted_sentences\n",
        "    "
      ],
      "metadata": {
        "id": "wWOBRmdrFBMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceSplitter(sentence_length=80,\n",
        "            lstm_layers_size= (64, 32),\n",
        "            embedding_layer_size = 100,\n",
        "            dropout_rate= 0.3,\n",
        "            activation= 'softmax',\n",
        "            epochs=10)\n",
        "results = SentenceSplitter.final_resut(summary)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-Z_QfpqFj36",
        "outputId": "fe2855aa-507d-42fd-c2c3-4c8064780410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The mountain's slopes are covered with forest mangoes, glowing with ripe fruit\", 'it takes on the appearance of a breast of the earth, dark at the centre and pale at the top', 'The mountain is covered in mangoes and mangoes in the summer.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keyword extractor"
      ],
      "metadata": {
        "id": "lnG_8oR1FrfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordwise import Extractor\n",
        "extractor = Extractor()\n",
        "i=\"https://docs.google.com/presentation/d/1gd08BOQzG4miaxPJZ_iZ7xSFPJ8IEllS/edit#slide=id.p1\"\n",
        "length = 3\n",
        "if(len(i)>=10):\n",
        "  length=4\n",
        "if(len(i)>20):\n",
        "    length=5\n",
        "keywords = extractor.generate(i,length)\n",
        "print(keywords)"
      ],
      "metadata": {
        "id": "JbXY3D_IFt_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}